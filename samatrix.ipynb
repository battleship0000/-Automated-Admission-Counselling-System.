{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2SFequdJ24xec91UtgQ9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/battleship0000/-Automated-Admission-Counselling-System./blob/main/samatrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Explain the role of weights in a neuron.\n",
        "# Ans.\n",
        " In a neural network weights determine the importance of an input.\n",
        "\n",
        "Think of a neuron like a decision-making filter:\n",
        "**The Inputs (x)**: The information coming in.\n",
        "**The Weights (w)**: How much you trust or value that specific information.\n",
        "**The Calculation**: Input X times = Weight = Influence.\n",
        "\n",
        "**The Rule of Thumb:**\n",
        "\n",
        "**High Weight**: This input is very important to the final result.\n",
        "\n",
        "**Low/Zero Weight**: This input is ignored or has no impact.\n",
        "\n",
        "**Negative Weight**: This input discourages the final result (it acts as a \"penalty\").\n",
        "\n",
        "**During Training:**\n",
        "\n",
        "\"Learning\" is simply the process of automatically adjusting these weights until the model consistently makes the right prediction."
      ],
      "metadata": {
        "id": "Vu35P1QXUoJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What is an activation function.\n",
        "# Ans.\n",
        " An activation function is a mathematical \"gatekeeper\" that decides whether a neuron should \"fire\" (pass a signal) or not.\n",
        "\n",
        "**Its 3 Main Roles:**\n",
        "\n",
        "**Decision Maker**: It takes the weighted sum of inputs and converts it into an output (usually between 0 and 1, or -1 and 1).\n",
        "\n",
        "**Non-Linearity**: This is the most important part. Without it, a neural network is just a simple linear regression. It allows the model to learn complex patterns like faces, voices, or human language.\n",
        "+1\n",
        "\n",
        "**Normalization**: It keeps the output values within a manageable range so the numbers don't explode toward infinity during calculations.\n",
        "\n",
        "**Common Examples:**\n",
        "\n",
        "ReLU (Most Popular): If the input is positive, pass it through; if negative, keep it at zero.\n",
        "\n",
        "Sigmoid: Squashes any value into a range between 0 and 1 (great for probability).\n",
        "\n",
        "Softmax: Used at the very end of a model to determine which category is the most likely winner."
      ],
      "metadata": {
        "id": "p1Kq4SFXUyEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define probability distribution in ML context.\n",
        "\n",
        "# Ans.\n",
        "A probability distribution is a mathematical function that describes the likelihood of all possible outcomes for a variable.\n",
        "\n",
        "In Machine Learning, it tells the model how \"certain\" it should be about a prediction.\n",
        "\n",
        "**Key Roles in ML**:\n",
        "\n",
        "Output Layer: In classification, the model outputs a distribution (e.g., 90% chance \"Cat,\" 10% chance \"Dog\").\n",
        "\n",
        "Data Modeling: We assume input data follows a specific pattern (like a Gaussian/Normal Distribution) to help the model learn more efficiently.\n",
        "\n",
        "Uncertainty: It helps the model quantify what it doesn't know.\n",
        "\n",
        "**Most Common Types:**\n",
        "\n",
        "Bernoulli: For binary (Yes/No) outcomes, like spam detection.\n",
        "\n",
        "Multinoulli: For multiple categories, like recognizing handwritten digits (0–9).\n",
        "\n",
        "Gaussian (Normal): The \"Bell Curve\" used for continuous data like heights or house prices."
      ],
      "metadata": {
        "id": "tMGRjm_1VpsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is gradient in optimization?\n",
        "\n",
        "# Ans.\n",
        "A gradient is the \"slope\" or \"steepness\" of a function at a specific point. It tells an optimization algorithm (like Gradient Descent) which way is \"uphill\" and how fast it’s rising.\n",
        "\n",
        "**Its 2 Vital Roles:**\n",
        "\n",
        "**Direction**:\n",
        "\n",
        " The gradient points toward the direction of the steepest increase. To minimize error (the goal of ML), we move in the opposite direction.\n",
        "\n",
        "**Magnitude:**\n",
        "\n",
        "A larger gradient means the slope is very steep (we are far from the goal); a tiny gradient means we are nearing the bottom (the \"minimum\").\n",
        "\n",
        "**The Optimization Logic:**\n",
        "\n",
        "\n",
        "**Calculate Gradient:** Find the slope of the error.\n",
        "\n",
        "**Update Weights:** Nudge the weights \"downhill\" (negative gradient).\n",
        "\n",
        "**Repeat**: Stop once the gradient is near zero, meaning you've reached the lowest possible error."
      ],
      "metadata": {
        "id": "3fQRSPI_W3fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with Tensors\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Prepare Data (Features: Study Hours, Target: Pass=1/Fail=0)\n",
        "X = torch.tensor([[1.0], [2.0], [3.0], [4.0], [5.0], [6.0]], dtype=torch.float32)\n",
        "y = torch.tensor([[0.0], [0.0], [0.0], [1.0], [1.0], [1.0]], dtype=torch.float32)\n",
        "\n",
        "# 2. Define Model Architecture\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1) # 1 Input -> 1 Output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x)) # Activation function\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "# 3. Loss (BCE) and Optimizer (SGD)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# 4. Training Loop (Adjusting Weights)\n",
        "for epoch in range(200):\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    optimizer.zero_grad() # Reset gradients\n",
        "    loss.backward()      # Calculate gradient\n",
        "    optimizer.step()     # Update weights\n",
        "\n",
        "# 5. Prediction\n",
        "test_hour = torch.tensor([[3.5]], dtype=torch.float32)\n",
        "prediction = model(test_hour)\n",
        "print(f\"Probability of passing with 3.5 hours: {prediction.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9S2EH8SXpnS",
        "outputId": "5e280eef-0695-4821-a621-0814d10b30ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of passing with 3.5 hours: 0.6169\n"
          ]
        }
      ]
    }
  ]
}